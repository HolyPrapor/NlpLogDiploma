{"nbformat":4,"nbformat_minor":5,"metadata":{"notebookPath":"NlpLogDiploma/Research/baseline_experiments.ipynb","language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py"},"notebookId":"abb0c358-bf44-4f45-b3d2-0cbad2121587","kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"}},"cells":[{"cell_type":"code","source":"%pip install -U transformers","metadata":{"cellId":"x282rwxe6rive6a1aogz8r","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: transformers in /home/jupyter/.local/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: importlib-metadata in /kernel/lib/python3.7/site-packages (from transformers) (4.8.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\nRequirement already satisfied: numpy>=1.17 in /kernel/lib/python3.7/site-packages (from transformers) (1.19.4)\nRequirement already satisfied: packaging>=20.0 in /kernel/lib/python3.7/site-packages (from transformers) (20.9)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.3.1)\nRequirement already satisfied: requests in /kernel/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from transformers) (0.1.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.50.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2021.7.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in /kernel/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyter/.local/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\nRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /kernel/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /kernel/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: six in /kernel/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n\u001B[33mWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\n"}],"execution_count":62},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2Model\ngpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntext = \"Oh my god why did you do\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = F.softmax(gpt2(**encoded_input, return_dict=True).logits[:, -1, :])\n\n# We are able to extract next token probs this way.\n\nfor token in reversed(torch.argsort(output)[0][-3:]):\n    print(tokenizer.decode(token))\n    print(output[0][token])","metadata":{"cellId":"m6sg5h61cd1o89ny9omgk","trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/kernel/lib/python3.7/site-packages/ml_kernel/kernel.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  \n"},{"output_type":"stream","name":"stdout","text":" that\ntensor(0.5987, grad_fn=<SelectBackward>)\n this\ntensor(0.2333, grad_fn=<SelectBackward>)\n it\ntensor(0.0974, grad_fn=<SelectBackward>)\n"}],"execution_count":72},{"cell_type":"code","source":"def extract_token_probs_from_text(text):\n    encoded_input = tokenizer(text, return_tensors='pt')\n    output = F.softmax(gpt2(**encoded_input, return_dict=True).logits[:, -1, :], dim=-1)\n    return {tokenizer.decode(i) : output[0][i] for i in range(len(output))}","metadata":{"cellId":"my6e7zjm6mqn7d8ulgox6","trusted":true},"outputs":[],"execution_count":78},{"cell_type":"code","source":"from time import time\n\nstart = time()\n\nfor i in range(100):\n    if i % 10 == 0:\n        print(f\"{i + 1}th iteration\".ljust(30) + f\"{(time() - start):.3f}\")\n    extract_token_probs_from_text(\"why did you do\")\n\nprint()\nprint(f\"Overall time: \".ljust(30) + f\"{(time() - start):.3f}\")","metadata":{"cellId":"tpn77jjk1ojrhs0klwo2bk","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"1th iteration                 0.000\n11th iteration                0.670\n21th iteration                1.341\n31th iteration                2.010\n41th iteration                2.672\n51th iteration                3.340\n61th iteration                4.027\n71th iteration                4.691\n81th iteration                5.360\n91th iteration                6.077\n\nOverall time:                 6.752\n"}],"execution_count":90},{"cell_type":"code","source":"# Мысль: во время энкодинга, кажется, можно глядеть сразу на несколько шагов вперёд, и отталкиваться от этой информации, чтобы оперировать батчами и ускорить процесс.\n# Во время декодинга, кажется, такое не выйдет :(","metadata":{"cellId":"i0huma6qhelm61re7a0u","trusted":true},"outputs":[],"execution_count":83}]}